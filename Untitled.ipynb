{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c55adb71-f0bf-40b9-8cf7-a333f1b32aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04098f47-381c-4cc9-8b38-9981343ef431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "\n",
    "INPUT_SIZE = 60\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS = 2\n",
    "OUTPUT_SIZE = 1\n",
    "\n",
    "# Hyper parameters\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68b7494b-1a4e-43a7-ae09-687db981a173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2002-02-13</td>\n",
       "      <td>6.480513</td>\n",
       "      <td>6.773399</td>\n",
       "      <td>6.413183</td>\n",
       "      <td>6.766666</td>\n",
       "      <td>19054000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2002-02-14</td>\n",
       "      <td>6.850831</td>\n",
       "      <td>6.864296</td>\n",
       "      <td>6.682506</td>\n",
       "      <td>6.733003</td>\n",
       "      <td>2755400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002-02-15</td>\n",
       "      <td>6.733001</td>\n",
       "      <td>6.749833</td>\n",
       "      <td>6.632006</td>\n",
       "      <td>6.699336</td>\n",
       "      <td>2097400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2002-02-19</td>\n",
       "      <td>6.665671</td>\n",
       "      <td>6.665671</td>\n",
       "      <td>6.312189</td>\n",
       "      <td>6.430017</td>\n",
       "      <td>1852600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2002-02-20</td>\n",
       "      <td>6.463681</td>\n",
       "      <td>6.648838</td>\n",
       "      <td>6.413183</td>\n",
       "      <td>6.648838</td>\n",
       "      <td>1723200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4882</th>\n",
       "      <td>2021-07-07</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>197.500000</td>\n",
       "      <td>177.559998</td>\n",
       "      <td>190.660004</td>\n",
       "      <td>4239500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4883</th>\n",
       "      <td>2021-07-08</td>\n",
       "      <td>179.830002</td>\n",
       "      <td>194.220001</td>\n",
       "      <td>179.500000</td>\n",
       "      <td>191.380005</td>\n",
       "      <td>2857500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4884</th>\n",
       "      <td>2021-07-09</td>\n",
       "      <td>190.880005</td>\n",
       "      <td>194.779999</td>\n",
       "      <td>186.750000</td>\n",
       "      <td>191.229996</td>\n",
       "      <td>1579100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4885</th>\n",
       "      <td>2021-07-12</td>\n",
       "      <td>191.419998</td>\n",
       "      <td>197.750000</td>\n",
       "      <td>187.419998</td>\n",
       "      <td>189.250000</td>\n",
       "      <td>1633600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4886</th>\n",
       "      <td>2021-07-13</td>\n",
       "      <td>187.679993</td>\n",
       "      <td>188.789993</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>180.059998</td>\n",
       "      <td>2371239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4887 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        Open        High         Low       Close    Volume  \\\n",
       "0     2002-02-13    6.480513    6.773399    6.413183    6.766666  19054000   \n",
       "1     2002-02-14    6.850831    6.864296    6.682506    6.733003   2755400   \n",
       "2     2002-02-15    6.733001    6.749833    6.632006    6.699336   2097400   \n",
       "3     2002-02-19    6.665671    6.665671    6.312189    6.430017   1852600   \n",
       "4     2002-02-20    6.463681    6.648838    6.413183    6.648838   1723200   \n",
       "...          ...         ...         ...         ...         ...       ...   \n",
       "4882  2021-07-07  196.000000  197.500000  177.559998  190.660004   4239500   \n",
       "4883  2021-07-08  179.830002  194.220001  179.500000  191.380005   2857500   \n",
       "4884  2021-07-09  190.880005  194.779999  186.750000  191.229996   1579100   \n",
       "4885  2021-07-12  191.419998  197.750000  187.419998  189.250000   1633600   \n",
       "4886  2021-07-13  187.679993  188.789993  179.000000  180.059998   2371239   \n",
       "\n",
       "      Dividends  Stock Splits  \n",
       "0           0.0           0.0  \n",
       "1           0.0           0.0  \n",
       "2           0.0           0.0  \n",
       "3           0.0           0.0  \n",
       "4           0.0           0.0  \n",
       "...         ...           ...  \n",
       "4882        0.0           0.0  \n",
       "4883        0.0           0.0  \n",
       "4884        0.0           0.0  \n",
       "4885        0.0           0.0  \n",
       "4886        0.0           0.0  \n",
       "\n",
       "[4887 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the training set\n",
    "dataset_train = pd.read_csv('gme_hist.csv')\n",
    "training_set = dataset_train.iloc[:, 1:2].values\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35dbbcbd-587f-405b-9a8d-56b97e5eb9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1620377a-ec5c-486e-8678-e3c8484a626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data structure with 60 timesteps and 1 output\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(INPUT_SIZE, 1258):\n",
    "    X_train.append(training_set_scaled[i-INPUT_SIZE:i, 0])\n",
    "    y_train.append(training_set_scaled[i, 0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdf8b6c1-a91d-4bc7-a74c-5394883f03b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e699c0ea-36c7-4f3e-a506-c5d0a272a5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, i_size, h_size, n_layers, o_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=i_size,\n",
    "            hidden_size=h_size,\n",
    "            num_layers=n_layers\n",
    "        )\n",
    "        self.out = nn.Linear(h_size, o_size)\n",
    "\n",
    "    def forward(self, x, h_state):\n",
    "        r_out, hidden_state = self.rnn(x, h_state)\n",
    "        \n",
    "        hidden_size = hidden_state[-1].size(-1)\n",
    "        r_out = r_out.view(-1, hidden_size)\n",
    "        outs = self.out(r_out)\n",
    "\n",
    "        return outs, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "858541e1-2aaf-4bb9-9dfa-12614d288685",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, i_size, h_size, n_layers, o_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=i_size,\n",
    "            hidden_size=h_size,\n",
    "            num_layers=n_layers\n",
    "        )\n",
    "        self.out = nn.Linear(h_size, o_size)\n",
    "\n",
    "    def forward(self, x, h_state):\n",
    "        r_out, hidden_state = self.rnn(x, h_state)\n",
    "        \n",
    "        hidden_size = hidden_state[-1].size(-1)\n",
    "        r_out = r_out.view(-1, hidden_size)\n",
    "        outs = self.out(r_out)\n",
    "\n",
    "        return outs, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efbe6bcf-fc52-4113-9e3e-b2291999d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, OUTPUT_SIZE)\n",
    "\n",
    "optimiser = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "hidden_state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2b84107-a75f-4336-82a2-47c698dd7093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.01038141455501318\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [64, 256]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-45b111bb3029>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0moptimiser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m                     \u001b[1;31m# back propagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0moptimiser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m                                     \u001b[1;31m# update the parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [64, 256]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    inputs = Variable(torch.from_numpy(X_train).float())\n",
    "    labels = Variable(torch.from_numpy(y_train).float())\n",
    "\n",
    "    output, hidden_state = rnn(inputs, hidden_state) \n",
    "\n",
    "    loss = criterion(output.view(-1), labels)\n",
    "    optimiser.zero_grad()\n",
    "    loss.backward(retain_graph=True)                     # back propagation\n",
    "    optimiser.step()                                     # update the parameters\n",
    "    \n",
    "    print('epoch {}, loss {}'.format(epoch,loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb4fb7a-3abb-4e62-bd96-4c3040bd457a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
