{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **This is an adaption of a GitHub notebook tha looks at Reddits on wallstreets bets. The Notebook is divided into two sections:**\n",
    "1. [Oringial Section](#1---Oringial-Section)\n",
    "\n",
    "that does the basic analsysis. We need to run this to set up the datastructures in the notebook\n",
    "\n",
    "2. [New Section](#2---New-Section)\n",
    "\n",
    "That builds on the data structures<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Oringial Section\n",
    "## **This is a initial descriptive analysis of the Reddit wallstreetbets posts. It contains a basic statistics of words, character count, and occurence. At the bottom, you will find the analysis of the most common mentioned NYSE or other stock tickers. Enjoy!**\n",
    "\n",
    "## section deteleted.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - New Section\n",
    "to combine Title and Body to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 Set up (From Original code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from datetime import date, datetime\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import stats\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\big_j\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# additional imports\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import datetime\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import alpaca_trade_api as tradeapi\n",
    "# from datetime import date, datetime\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required Keras modules\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpaca_trade_api.rest import TimeFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Read file and check file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's not about the money, it's about sending a...</td>\n",
       "      <td>55</td>\n",
       "      <td>l6ulcx</td>\n",
       "      <td>https://v.redd.it/6j75regs72e61</td>\n",
       "      <td>6</td>\n",
       "      <td>1611862661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/28/2021 21:37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Math Professor Scott Steiner says the numbers ...</td>\n",
       "      <td>110</td>\n",
       "      <td>l6uibd</td>\n",
       "      <td>https://v.redd.it/ah50lyny62e61</td>\n",
       "      <td>23</td>\n",
       "      <td>1611862330</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/28/2021 21:32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title score      id  \\\n",
       "0  It's not about the money, it's about sending a...    55  l6ulcx   \n",
       "1  Math Professor Scott Steiner says the numbers ...   110  l6uibd   \n",
       "\n",
       "                               url comms_num     created body  \\\n",
       "0  https://v.redd.it/6j75regs72e61         6  1611862661  NaN   \n",
       "1  https://v.redd.it/ah50lyny62e61        23  1611862330  NaN   \n",
       "\n",
       "         timestamp Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11  \n",
       "0  1/28/2021 21:37        NaN        NaN         NaN         NaN  \n",
       "1  1/28/2021 21:32        NaN        NaN         NaN         NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for testing delete\n",
    "df = pd.read_csv('./reddit_wsb.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Drop useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>body</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's not about the money, it's about sending a...</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/28/2021 21:37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Math Professor Scott Steiner says the numbers ...</td>\n",
       "      <td>110</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/28/2021 21:32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title score comms_num body  \\\n",
       "0  It's not about the money, it's about sending a...    55         6  NaN   \n",
       "1  Math Professor Scott Steiner says the numbers ...   110        23  NaN   \n",
       "\n",
       "         timestamp Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11  \n",
       "0  1/28/2021 21:37        NaN        NaN         NaN         NaN  \n",
       "1  1/28/2021 21:32        NaN        NaN         NaN         NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for testing delete\n",
    "df = df.drop(columns=['id', 'url', 'created'])\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Add a few data realted columns for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data '1/28/2021 21:37' does not match format '%Y-%m-%d %H:%M:%S'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-7bdf68f7d385>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mminute_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'timestamp'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'%Y-%m-%d %H:%M:%S'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0myear_col\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mmonth_col\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\_strptime.py\u001b[0m in \u001b[0;36m_strptime_datetime\u001b[1;34m(cls, data_string, format)\u001b[0m\n\u001b[0;32m    566\u001b[0m     \"\"\"Return a class cls instance based on the input string and the\n\u001b[0;32m    567\u001b[0m     format string.\"\"\"\n\u001b[1;32m--> 568\u001b[1;33m     \u001b[0mtt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfraction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgmtoff_fraction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_strptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m     \u001b[0mtzname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgmtoff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfraction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\_strptime.py\u001b[0m in \u001b[0;36m_strptime\u001b[1;34m(data_string, format)\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[0mfound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformat_regex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         raise ValueError(\"time data %r does not match format %r\" %\n\u001b[0m\u001b[0;32m    350\u001b[0m                          (data_string, format))\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mfound\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: time data '1/28/2021 21:37' does not match format '%Y-%m-%d %H:%M:%S'"
     ]
    }
   ],
   "source": [
    "# for testing delete\n",
    "from datetime import date, datetime\n",
    "year_col = []\n",
    "month_col = []\n",
    "hour_col = []\n",
    "minute_col = []\n",
    "for i, content in df['timestamp'].items():\n",
    "    t1 = datetime.strptime(content, '%Y-%m-%d %H:%M:%S')\n",
    "    year_col.append(t1.year)\n",
    "    month_col.append(t1.month)\n",
    "    hour_col.append(t1.hour)\n",
    "    minute_col.append(t1.minute)\n",
    "df['year'] = year_col\n",
    "df['month'] = month_col\n",
    "df['hour'] = hour_col\n",
    "df['minute'] = minute_col\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the text to be lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing delete\n",
    "df['title'] = df['title'].str.lower()\n",
    "df['body'] = df['body'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing delete\n",
    "df.drop_duplicates(subset=['title'], keep='first', inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Descriptive statistics - title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count the number of characters and length of a title**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing delete\n",
    "count = df['title'].str.split().str.len()\n",
    "count.index = count.index.astype(str) + ' words:'\n",
    "count.sort_index(inplace=True)\n",
    "\n",
    "print(\"Total number of words: \", count.sum(), \"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing delete\n",
    "print(\"Average number of words per post: \", round(count.mean(),2), \"words\")\n",
    "print(\"Max number of words per post: \", count.max(), \"words\")\n",
    "print(\"Min number of words per post: \", count.min(), \"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(df):\n",
    "    \"\"\"\n",
    "    This function takes the dataframe and adds a new colun with the number of words.\n",
    "    :param df: The dataframe to be transformed.\n",
    "    :return: The transformed dataframe.\n",
    "    \"\"\"\n",
    "    words_count = []\n",
    "    for i, content in df['title'].items():\n",
    "        new_values =[]\n",
    "        new_values = content.split()\n",
    "        words_count.append(len(new_values))\n",
    "    df['title_word_count'] = words_count\n",
    "    return df\n",
    "\n",
    "# df = word_count(df)\n",
    "\n",
    "# df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing delete\n",
    "df['title_length'] = df['title'].str.len()\n",
    "\n",
    "print(\"Total length of a dataset: \", df.title_length.sum(), \"characters\")\n",
    "print(\"Average length of a tweet: \", round(df.title_length.mean(),0), \"characters\")\n",
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for testing delte\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 function to get data and clean up in two function\n",
    "First Function to read from file. This can be replace by reading real data<br>\n",
    "Second Function to combine and clean the data as in functions above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **start testing here after loading libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start testing here\n",
    "df_raw = pd.read_csv('./reddit_wsb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(in_df):\n",
    "    #clean data and return a \"standrd form\"\n",
    "    # input input data frame as describe above\n",
    "    # output: cleand data frame\n",
    "    from datetime import date, datetime # get librareis\n",
    "    df = in_df.copy() # make a copy\n",
    "    df = df.drop(columns=['id', 'url', 'created']) # drop unused columns\n",
    "    \n",
    "    #add date and time\n",
    "    year_col = []\n",
    "    month_col = []\n",
    "    hour_col = []\n",
    "    minute_col = []\n",
    "    day_col = [] # v 1.6 add day colummn\n",
    "    date_col = [] #\n",
    "    for i, content in df['timestamp'].items():\n",
    "        t1 = datetime.strptime(content, '%Y-%m-%d %H:%M:%S')\n",
    "        year_col.append(t1.year)\n",
    "        month_col.append(t1.month)\n",
    "        hour_col.append(t1.hour)\n",
    "        minute_col.append(t1.minute)\n",
    "        day_col.append(t1.day) #v 1.6 add day column\n",
    "        date_col.append(t1.date())\n",
    "    df['year'] = year_col\n",
    "    df['month'] = month_col\n",
    "    df['hour'] = hour_col\n",
    "    df['minute'] = minute_col \n",
    "    df['day'] = day_col # v 1.6 add day col\n",
    "    df['date'] = date_col\n",
    "    \n",
    "    #Normalize the text to be lowercase\n",
    "    df['title'] = df['title'].str.lower()\n",
    "    df['body'] = df['body'].str.lower()\n",
    "    df.drop_duplicates(subset=['title'], keep='first', inplace=True)\n",
    "\n",
    "    #Count the number of characters and length of a title\n",
    "    df = word_count(df)\n",
    "    df['title_length'] = df['title'].str.len()\n",
    "    \n",
    "    #return value\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug delete\n",
    "df = clean_data(df_raw)\n",
    "df.head(2)\n",
    "# datetime.strptime(content, '%Y-%m-%d %H:%M:%S')\n",
    "# test_ymd = datetime.strptime(df['timestamp'][0], '%Y-%m-%d %H:%M:%S')\n",
    "# test_ymd.day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2, Combine Title and Body (start of new code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug delete\n",
    "# make a copy so original is not damanged and drop where there is no text in title or body\n",
    "new_df = df[~(df['title'].isna() & df ['body'].isna() )].copy()\n",
    "# convertitle and body to string and concaterante\n",
    "new_df ['title_body'] = df['title'].astype(str) + ' ' + df ['body'].astype(str)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 combine title and body in to one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_columns(in_df, in_col1, in_col2):\n",
    "    #combine two columns into one\n",
    "    #convert columns to text and concat\n",
    "    #input:\n",
    "    #in_df - datafraem\n",
    "    #in_col1 - name of column 1\n",
    "    #in_col2 - name of column 2\n",
    "    #return data_frame with extract column\n",
    "    \n",
    "#     # make a copy so original is not damanged and drop where there is no text in title or body\n",
    "\n",
    "    # convertitle and body to string and concaterante\n",
    "    new_df = in_df.copy()\n",
    "    new_df [in_col1 + '_' + in_col2] = in_df[in_col1].astype(str) + ' ' + in_df [in_col2].astype(str)\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing delete\n",
    "test_df = combine_columns(df, 'title', 'body')\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 routine to do count of ticker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_comment_count(in_df, in_word_list, in_column): \n",
    "    # purpose: finds count of words in a column and creates named coumns with counts\n",
    "    #inputs: \n",
    "    #in_df - input dataframe \n",
    "    #in_word_list list of words to find\n",
    "    #in_column the column in which the wordsare\n",
    "    #output:\n",
    "    #a copy of the original data with columns added for word count\n",
    "    \n",
    "    #code here\n",
    "    #put in error handler\n",
    "    try:\n",
    "        tmp_df = in_df.copy() #make a copy of the dataframe\n",
    "        tmp_df [in_column] = tmp_df [in_column].astype(str) # convrt to string\n",
    "        srch_word_list = [str(x) for x in in_word_list] #convert everthing words list to string\n",
    "        #wordList = re.sub(\"[^\\w]\", \" \",  mystr).split()\n",
    "        tmp_df[in_column + '_list'] = tmp_df[in_column].apply (lambda x: re.sub(\"[^\\w]\", \" \",  x).split()) # convert into a list of words\n",
    "        \n",
    "        # loop through list of words and create counts\n",
    "        for cur_word in srch_word_list:\n",
    "            # v 1.8 dont look at list,  look in original column with \n",
    "            #tmp_df [cur_word + '_count'] = tmp_df[in_column + '_list'].apply(lambda x: x.count(cur_word))\n",
    "            lower_word = cur_word.lower()# v 1.8\n",
    "            tmp_df [cur_word + '_count'] = tmp_df[in_column].apply(lambda x: x.count(lower_word)) # v 1.8\n",
    "        return tmp_df\n",
    "        \n",
    "    except:\n",
    "        #if eror then return null\n",
    "        return  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete\n",
    "test_df = find_comment_count(test_df, ['GME'], 'title_body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 Get emotions from each combined message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download/Update the VADER Lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 function to add vader emotion to each message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate vader sentiment\n",
    "def add_vader_compound(in_df, in_column, in_tick_mention, in_tick):\n",
    "    #take and input column, add vader sentiment\n",
    "    # input:\n",
    "    # in_df  - put put df\n",
    "    # in_column - text for which \n",
    "    # in_tick_mention - number of times ticker \n",
    "    # in_tick - name for the ticke\n",
    "    #output:\n",
    "    #df with two addition coumnns\n",
    "    #additional col1 = in_column + '_sent' - sentiment for the in_column\n",
    "    #           cik2 = in_tick_sent - sentiment where ticker is mentioned\n",
    "    \n",
    "    \n",
    "    \n",
    "    # make sure nltk and vader is downloaded \n",
    "    # if not downloaded, uncomment and download\n",
    "    # Download/Update the VADER Lexicon\n",
    "    #nltk.download('vader_lexicon')\n",
    "    #from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    # Initialize the VADER sentiment analyzer\n",
    "    # analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    \n",
    "#     nltk.download('vader_lexicon')\n",
    "\n",
    "\n",
    "     try:\n",
    "        tmp_df = in_df.copy() #make a copy of the dataframe\n",
    "        tmp_df [in_column] = tmp_df [in_column].astype(str) # convrt to string\n",
    "        analyzer = SentimentIntensityAnalyzer() # set analyzer\n",
    "        #total sentiment\n",
    "        tot_sent = in_column + '_sent'\n",
    "        tmp_df[tot_sent] = tmp_df [in_column].apply (lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "        #sentiment on ticker mention\n",
    "        tmp_df[in_tick + '_sent'] = tmp_df.apply(lambda x: x[in_tick_mention] if x[in_tick_mention] == 0 else x[tot_sent], axis = 1)\n",
    "            \n",
    "        return tmp_df\n",
    "        \n",
    "     except:\n",
    "         #if eror then return null\n",
    "         print ('error in add_vader_compound')\n",
    "         return \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete\n",
    "# test code for vader\n",
    "# test= test_df.copy()\n",
    "in_tick = 'GME'\n",
    "test1_df = add_vader_compound(test_df, 'title_body', in_tick + '_count',in_tick)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_df = test1_df.copy()\n",
    "tick = 'GME'\n",
    "tick_sent = tick + '_sent'\n",
    "tick_count = tick + '_count'\n",
    "# type(test2_df[tick_count][0])\n",
    "# test1_df['up_neu_dn'] = test1_df ['pct_ch'].apply (lambda x: -1 if x < -zero_rng else 1 if x > zero_rng else 0)\n",
    "test2_df[tick_sent] = test2_df.apply (lambda x: x[tick_count] if x[tick_count] == 0 else x['title_body_sent'], axis = 1)\n",
    "# test2_df[tick_sent] = test2_df.apply (lambda x: x['GME_count'], axis = 1)\n",
    "test2_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dletee\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "test['sent'] = test['title_body'].apply (lambda x: analyzer.polarity_scores(x)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete\n",
    "test.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 get stock price at that time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prices (in_tick, in_strt_date, in_end_date,in_timeframe, in_env_path):\n",
    "    # return a dataframe with stock price info from Alpaca by the minute\n",
    "    # inputs:\n",
    "    # in_tick - string with ticker symbol\n",
    "    # in_start_date as a start date in format: beg_date = '2021-01-05'\n",
    "    # in_end_date in format: end_date = '2021-01-05'\n",
    "    # in_timeframe set as interval for stock price\n",
    "    # in_env_path path for enviornment file\n",
    "    # return:\n",
    "    # data frame with stock prices.\n",
    "    # To do - put output structure\n",
    "    \n",
    "    #note works for 1000 items. how to get longer times.\n",
    "    from alpaca_trade_api.rest import TimeFrame\n",
    "    try:\n",
    "        #load environment\n",
    "        load_dotenv(in_env_path)\n",
    "        \n",
    "        #get alpaka keys\n",
    "        alpaca_api_key = os.getenv(\"ALPACA_API_KEY\")\n",
    "        alpaca_secret_key = os.getenv(\"ALPACA_SECRET_KEY\")\n",
    "#         print (f'alpaca api {alpaca_api_key}\\n')\n",
    "#         print (f'alpaca secret {alpaca_secret_key}\\n')\n",
    "        #set up alpaca\n",
    "        api = tradeapi.REST(\n",
    "        alpaca_api_key,\n",
    "        alpaca_secret_key,\n",
    "        api_version = \"v2\"\n",
    "        )\n",
    "    \n",
    "        start =  pd.Timestamp(f'{in_strt_date} 09:30:00-0400', tz='America/New_York').replace(hour=9, minute=30, second=0).astimezone('GMT').isoformat()[:-6]+'Z'\n",
    "        end   =  pd.Timestamp(f'{in_end_date} 16:00:00-0400', tz='America/New_York').replace(hour=16, minute=0, second=0).astimezone('GMT').isoformat()[:-6]+'Z'\n",
    "#         print (f'start: {start}\\n')\n",
    "#         print (f'end: {end}\\n')\n",
    "#         prices = api.get_barset(in_tick, TimeFrame.Hours,limit=1000, start=start, end=end).df\n",
    "        prices = api.get_barset(in_tick, in_timeframe,limit=1000, start=start, end=end).df\n",
    "        \n",
    "        #add date colum\n",
    "        prices['date'] = prices.index.map(lambda x: x.date())\n",
    "        \n",
    "        return prices\n",
    "\n",
    "    except:\n",
    "        return null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete\n",
    "from alpaca_trade_api.rest import TimeFrame\n",
    "price_df = get_prices('GME', '2021-01-28', '2021-06-01', '1D','test1.env' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete\n",
    "price_df.index[0].date()\n",
    "test_df = price_df.copy()\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 combine price with data and emotion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def comb_sent_mention(in_df, in_tick, in_timeframe,in_start_date, in_end_date, in_env_file, in_col1, in_col2 ):\n",
    "    #creates a combined data frame for a from data struct\n",
    "    # input:\n",
    "    # in_df = input data frame\n",
    "    # in_tick =  ticker symbo\n",
    "    # in_timeframe = suggest '1D'\n",
    "    # in_start_date = in the form'2021-01-28'\n",
    "    # in_end_date = in the form '2021-01-28'\n",
    "    # in_env_file = in this case'test1.env' \n",
    "    # in_col1 = in this case 'title'\n",
    "    # in_col2 = in this case 'body'   \n",
    "    # output:\n",
    "    # cobined dta frame\n",
    "\n",
    "#     price_df = get_prices(in_tick, in_start_date, in_end_date, in_timeframe,in_env_file) # get price from alpaa - not used here..\n",
    "    df_clean = clean_data(in_df) # clean it up\n",
    "    df = combine_columns(df_clean, in_col1, in_col2) # combine title and body\n",
    "    comb_col_title = in_col1 + '_' + in_col2 # combine the column titles\n",
    "    df = find_comment_count(df, [in_tick], comb_col_title) # find number of mentions \n",
    "    df = add_vader_compound(df, comb_col_title, in_tick + '_count', in_tick) # add emotion 2.5 updated\n",
    "# test1_df = add_vader_compound(test_df, 'title_body', in_tick + '_count',in_tick)    \n",
    "    #v 2.4 make column that is sentiment only if ticker is mentioend.\n",
    "    # ticker _ count\n",
    "    # title_body_sent\n",
    "    # \n",
    "    #sum sentiment and count by day - future fix add variable length\n",
    "    \n",
    "    grpby_cols = ['year', 'month', 'day']\n",
    "    sum_by_cols = [in_tick + '_count', comb_col_title + '_sent', in_tick + '_sent' ] # 2.5; summarize all ticker sentimetn\n",
    "    \n",
    "    tmp_df = df.groupby(grpby_cols)[sum_by_cols].sum().add_suffix('_sum')\n",
    "    df = df.join (tmp_df, on = grpby_cols)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete\n",
    "in_df = pd.read_csv('./reddit_wsb.csv')\n",
    "test_df = comb_sent_mention(in_df, 'GME', '1D', '2021-01-28','2021-06-28' , 'test1.env' ,  'title', 'body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete\n",
    "test_df.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 combine with daily price of stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe of only date, sentiment, world count, price, volueme mentions\n",
    "in_tick = 'GME'\n",
    "in_tick_count = in_tick + '_count_sum'\n",
    "in_tick_sent_sum = in_tick + '_sent_sum'\n",
    "new_df = test_df[['date', in_tick_count, 'title_body_sent_sum', in_tick_sent_sum]].copy()\n",
    "new_df.drop_duplicates(inplace = True)\n",
    "new_df.rename (columns = {'GME_count_sum': 'mentions', 'title_body_sent_sum': 'sentiment', in_tick_sent_sum: 'ticker_sent' }, inplace = True)\n",
    "new_df.set_index('date', inplace = True)\n",
    "new_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_prices = price_df [['date']]\n",
    "# new_prices = price_df[price_df.loc['GME']['close']]\n",
    "# delete not needed in testing\n",
    "new_prices = price_df ['GME']\n",
    "new_prices ['date'] = price_df['date']\n",
    "new_prices.set_index('date', inplace = True)\n",
    "new_prices.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df = get_prices('GME', '2021-01-28', '2021-06-01', '1D','test1.env' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_prices = price_df.copy()\n",
    "\n",
    "new_prices = pd.DataFrame()\n",
    "# new_prices['close']  = price_df['GME', 'close'].copy()\n",
    "# new_prices['volume'] = price_df['GME', 'close'].copy()\n",
    "# new_prices['date'] = new_prices.index.map(lambda x: x.date())\n",
    "# new_prices.reset_index(inplace = True)\n",
    "# new_prices.drop (columns = 'time', inplace = True)\n",
    "new_prices = price_df ['GME'].copy()\n",
    "new_prices['date'] = new_prices.index.map(lambda x: x.date())\n",
    "new_prices.reset_index(inplace = True)\n",
    "new_prices.drop (columns = ['time', 'open', 'high', 'low'], inplace = True)\n",
    "new_prices.set_index('date', inplace = True) \n",
    "new_prices.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = new_prices.join (new_df, how = 'left', on = 'date')\n",
    "output_df ['stock'] = 'GME'\n",
    "# output_df.head()\n",
    "output_df.shape\n",
    "output_df.head()\n",
    "# output_df.to_csv('sample_gme.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 Start ML part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Function to create combined dataframe for macine learning\n",
    "1. take company "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_data(df, window, feature_col_number, target_col_number):\n",
    "    \"\"\"\n",
    "    This function accepts the column number for the features (X) and the target (y).\n",
    "    It chunks the data up with a rolling window of Xt - window to predict Xt.\n",
    "    It returns two numpy arrays of X and y.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(df) - window):\n",
    "        features = df.iloc[i : (i + window), feature_col_number]\n",
    "        target = df.iloc[(i + window), target_col_number]\n",
    "        X.append(features)\n",
    "        y.append(target)\n",
    "    return np.array(X), np.array(y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy.\n",
    "test1_df = output_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_df ['pct_ch'] = test1_df['close'].pct_change()\n",
    "zero_rng = .01\n",
    "test1_df['up_neu_dn'] = test1_df ['pct_ch'].apply (lambda x: -1 if x < -zero_rng else 1 if x > zero_rng else 0)\n",
    "test1_df ['pct_chg_l1'] = test1_df ['pct_ch'].shift(1)\n",
    "test1_df ['up_neu_dn_l1'] = test1_df ['up_neu_dn'].shift(1)\n",
    "test1_df.dropna(inplace = True)\n",
    "test1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test1_df.iloc[:, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift_df = test1_df.shift(1)\n",
    "# shift_df\n",
    "# create a funtion to do this later\n",
    "win_size = 5\n",
    "feat_col = 4\n",
    "targ_col = 6\n",
    "X, y =  window_data(test1_df, win_size, feat_col, targ_col)\n",
    "# print (f\"X sample values:\\n{X[:5]} \\n\")\n",
    "# print (f\"y sample values:\\n{y[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 70% of the data for training and the remainder for testing\n",
    "split = int(0.7 * len(X))\n",
    "X_train = X[: split]\n",
    "X_test = X[split:]\n",
    "y_train = y[: split]\n",
    "y_test = y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the MinMaxScaler to scale data between 0 and 1.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "scaler.fit(y)\n",
    "y_train = scaler.transform(y_train)\n",
    "y_test = scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the features for the model\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "# print (f\"X_train sample values:\\n{X_train[:5]} \\n\")\n",
    "# print (f\"X_test sample values:\\n{X_test[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required Keras modules\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM RNN model.\n",
    "model = Sequential()\n",
    "\n",
    "number_units = 5\n",
    "dropout_fraction = 0.2\n",
    "\n",
    "# Layer 1\n",
    "model.add(LSTM(\n",
    "    units=number_units,\n",
    "    return_sequences=True,\n",
    "    input_shape=(X_train.shape[1], 1))\n",
    "    )\n",
    "model.add(Dropout(dropout_fraction))\n",
    "# Layer 2\n",
    "model.add(LSTM(units=number_units, return_sequences=True))\n",
    "model.add(Dropout(dropout_fraction))\n",
    "# Layer 3\n",
    "model.add(LSTM(units=number_units))\n",
    "model.add(Dropout(dropout_fraction))\n",
    "# Output layer\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=10, shuffle=False, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some predictions\n",
    "predicted = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover the original prices instead of the scaled version\n",
    "predicted_prices = scaler.inverse_transform(predicted)\n",
    "real_prices = scaler.inverse_transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of Real and Predicted values\n",
    "stocks = pd.DataFrame({\n",
    "    \"Real\": real_prices.ravel(),\n",
    "    \"Predicted\": predicted_prices.ravel()\n",
    "    }, index = test1_df.index[-len(real_prices): ])\n",
    "stocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the real vs predicted prices as a line chart\n",
    "stocks.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
