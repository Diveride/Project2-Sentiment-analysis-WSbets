{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env enviroment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load your .env file\n",
    "load_dotenv(\"/Users/ludovicschneider/Bootcamp/LS.env\")\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# Import Pushift API\n",
    "from psaw import PushshiftAPI\n",
    "api = PushshiftAPI()\n",
    "\n",
    "# Import requests\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st API: Retrieving the data via the Pushift Database ##\n",
    "This is a better way to extract large amount of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pushiftapi (subreddit, start_year, start_month, start_date, end_year, end_month, end_date, max_posts):\n",
    "    '''Returns pd.DataFrame with psts for the given time-window and subbreddit from Pushift API/database\n",
    "    ...\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subreddit : str() - name of the subreddit you want to fetch the data from - example : wallstreetbets\n",
    "    start_year : int() - year in the format YYYY\n",
    "    start_month : int() - year in the format MM\n",
    "    start_date : int() - year in the format DD\n",
    "    end_year : int() - year in the format YYYY\n",
    "    end_month : int() - year in the format MM\n",
    "    end_date : int() - year in the format DD \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df_clean : pd.DataFrame with list of posts with the attributes ''title', 'score', 'body', 'timestamp', 'author'' price data\n",
    "    Print number of row + start and end time\n",
    "    generate a CSV file\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Set the parametters to fetch the data from the database\n",
    "    #Set teh time window\n",
    "    start_time= int(dt.datetime(start_year,start_month,start_date).timestamp())\n",
    "    end_time= int(dt.datetime(end_year,end_month,end_date).timestamp())\n",
    "\n",
    "    # Set the request by specifying the filters/data we want to extract, \n",
    "    # in our case we will focus on Author - Title - Selftext(body) - score\n",
    "    # the score will be always 1 because this database contain only the fist submission/post and hence don't have the following \"events\" (comments/votes)\n",
    "\n",
    "    wsb_data = api.search_submissions(after=start_time, before=end_time,\n",
    "                                      subreddit= 'wallstreetbets',\n",
    "                                      filter=['author', 'title', 'selftext', 'score'],\n",
    "                                      limit= 200000\n",
    "                                     )\n",
    "    # Check that we have no errors\n",
    "    print(wsb_data)\n",
    "    \n",
    "    # Use \"thing.d_\" a dictionary containing all of the data attributes attached to the variable called \"thing\" \n",
    "    # (which otherwise would be accessed via dot notation). \n",
    "    # This is a quick way to pushing results into a pandas dataframe \n",
    "\n",
    "    df = pd.DataFrame([thing.d_ for thing in wsb_data])\n",
    "    \n",
    "    # transforming utc_date\n",
    "    df_clean = df.drop(columns='created').rename(columns={'created_utc':'timestamp', 'selftext':'body'})\n",
    "    df_clean['timestamp'] = df_clean['timestamp'].apply(lambda x: dt.datetime.utcfromtimestamp(x))\n",
    "    df_clean = df_clean[['title', 'score', 'body', 'timestamp', 'author']]\n",
    "    \n",
    "    # Creating the CSV file for the data analysis\n",
    "    df_clean.to_csv('wsb_pushshift_data.csv')\n",
    "    \n",
    "    # Print results\n",
    "    print(f'Shape of the Data_Frame : {df_clean.shape(1)}')\n",
    "    print(f'Start window : {df_clean.timestamp.iat[0]}')\n",
    "    print(f'End window : {df_clean.timestamp.iat[-1]}')\n",
    "    \n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object PushshiftAPIMinimal._search at 0x7fcc1b72eed0>\n",
      "(200000, 5)\n",
      "start window : 2021-07-10 03:59:25\n",
      "end window : 2021-03-29 09:37:35\n"
     ]
    }
   ],
   "source": [
    "pushiftapi('wallstreetbets', 2021, 1, 1, 2021, 7, 10, 200000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd API: Retrieving the data via Reddit API directly ##\n",
    "It is not the best way to retrieve large dataset but it is a good way to stream real time and to keep updates on posts. As exlpain, the first API doesn't \"update\" to take into accounts the comments and votes of a post, it only retrieves the intial post/submission.\n",
    "\n",
    "We are not going to use this API in our example but we want to give teh user teh flexibility to actually retrieve all teh information if you want to includes comments/votes information. \n",
    "\n",
    "The constraints of this API is that you are limited to 100 posts per requests. Hence we had to create a loop to allow the user to go back in time as much  as desired. However this limitation makes it harder/longer to build a large dataset to work on for our machine learning exercise. \n",
    "\n",
    "Therefore this part will be commented out but can be activated if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit_direct_api(subreddit, max_batch, limit_posts):\n",
    "    ''' Returns a pd.DataFrame with posts fetched from the direct Reddit API. \n",
    "        Reddit API lets you fetch only 100 posts at a time and hence you need to specify how many batchs of 100\n",
    "        you want to use to build your DF\n",
    "        \n",
    "        To run the function you need Reddit API keys and your Reddit account password saved in an .env\n",
    "        format : \n",
    "                client_id = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "                reddit_secret_key = os.getenv(\"REDDIT_SECRET_KEY\")\n",
    "                reddit_pw = os.getenv('REDDIT_PW')\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subreddit : str() - name of the subreddit you want to fetch the data from - example : wallstreetbets\n",
    "    max_batch: int() - max of loops we want to fetch (will determine how far back you want to go)\n",
    "                     (note two woudl be equal to 3 batches)\n",
    "\n",
    "    limit_posts : int() - max posts to retrieve per batch\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df_reddit : pd.DataFrame with list of posts with the attributes ''title', 'score', 'body', 'timestamp', 'author'' price data\n",
    "    Print number of row + start and end time\n",
    "    generate a CSV file\n",
    "    '''\n",
    "    \n",
    "    # Set Reddit API public and secret keys + Reddit account password\n",
    "    # We need to Request a temporary OAuth token from Reddit to access the API,\n",
    "    # to do so we need our keys and password:\n",
    "\n",
    "    client_id = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "    reddit_secret_key = os.getenv(\"REDDIT_SECRET_KEY\")\n",
    "    reddit_pw = os.getenv('REDDIT_PW')\n",
    "\n",
    "    # Prepare the authorization\n",
    "    auth = requests.auth.HTTPBasicAuth(client_id, reddit_secret_key)\n",
    "    \n",
    "    # Setting up your Reddit logins\n",
    "    data_id = {'grant_type': 'password',\n",
    "               'username': 'diveride',\n",
    "               'password': reddit_pw\n",
    "              }\n",
    "    # As per Reddit API doc, we create the headers needed to access the website\n",
    "    headers = {'User-Agent': 'API_project'}\n",
    "\n",
    "    # Retrieve the access_token\n",
    "    res = requests.post('https://www.reddit.com/api/v1/access_token', auth=auth, data=data_id, headers=headers)\n",
    "\n",
    "    # Store the access_token to a variable\n",
    "    access_token = res.json()['access_token']\n",
    "    \n",
    "    # Adding the access token to our headers and format it as a string\n",
    "    headers['authorization'] = f'bearer {access_token}'\n",
    "\n",
    "    # Print the access_token to make sure it worked in a proper format\n",
    "    print(f'Access Token confirmation:{headers}')\n",
    "    \n",
    "    # Checking that we can access the website with a result 200\n",
    "    # Important - the access Token is only valid for an hour\n",
    "    requests.get('https://oauth.reddit.com/api/v1/me', headers=headers)\n",
    "\n",
    "    # Retreiving the data from WSB\n",
    "    # This is where we created the loop function allowing you to go back as much as needed by batches of 100 posts(max authorized by the API)\n",
    "    # Since by default the request function will fetch the last 100 posts, we need to force the request to start before a specific post ID\n",
    "    # hence we create a last_id varable which will set the start of teh next batch\n",
    "    last_id=\"\"\n",
    "\n",
    "    # Set the \"token to keep track on number of batches\"\n",
    "    num=float()\n",
    "\n",
    "    # The loop code\n",
    "    df_res= pd.DataFrame()\n",
    "    while num <=max_batch:\n",
    "        num += 1\n",
    "        wsb_data1 = requests.get('https://oauth.reddit.com/r/'+ subreddit, headers=headers, params={'limit': limit_posts, 'after': 'last_id'})\n",
    "        for post in wsb_data1.json()['data']['children']:\n",
    "            df_res= df_res.append({\n",
    "                'author': post['data']['author'],\n",
    "                'title': post['data']['title'],\n",
    "                'selftext': post['data']['selftext'],\n",
    "                'score': post['data']['score'],\n",
    "                'time': post['data']['created_utc']\n",
    "            }, ignore_index=True)\n",
    "        last_id= post['kind'] + '_' + post['data']['id']\n",
    "        \n",
    "    # Fomating the unix time into days\n",
    "    df_res['time'] = df_res['time'].apply(lambda x: dt.datetime.utcfromtimestamp(x))\n",
    "    df_res = df_res.rename(columns={'time':'timestamp', 'selftext':'body'})\n",
    "    df_reddit = df_res[['title', 'score', 'body', 'timestamp', 'author']]\n",
    "    \n",
    "    # Creating the CSV file for the data analysis\n",
    "    df_reddit.to_csv('wsb_reddit_api_data.csv')\n",
    "    \n",
    "    # Print results\n",
    "    print(df_reddit.shape)\n",
    "    print(f'Start window : {df_reddit.timestamp.iat[0]}')\n",
    "    print(f'End window : {df_reddit.timestamp.iat[-1]}')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access Token confirmation:{'User-Agent': 'API_project', 'authorization': 'bearer 656422675188-fJPMrpxP__rI-eM5_7jXJU8hYxUSnA'}\n",
      "(1122, 5)\n",
      "start window : 2021-07-09 20:00:17\n",
      "end window : 2021-07-08 18:56:30\n"
     ]
    }
   ],
   "source": [
    "reddit_direct_api('wallstreetbets', 10, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:finance] *",
   "language": "python",
   "name": "conda-env-finance-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
